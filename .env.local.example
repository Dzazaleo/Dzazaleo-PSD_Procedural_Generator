# =============================================================================
# AI Provider Configuration (Qwen3-VL via Ollama)
# =============================================================================

# Ollama Server URL (OpenAI-compatible endpoint)
# Default: http://localhost:11434/v1 for Ollama
VITE_QWEN_BASE_URL=http://localhost:11434/v1

# Model identifier (must match the model pulled in Ollama)
# Options:
#   - qwen3-vl:8b   (Best quality/VRAM ratio, ~6.1GB weights, needs Ollama 0.12.7+)
#   - qwen2.5vl:7b  (Legacy fallback, ~6GB weights)
#   - minicpm-v:8b   (Lightweight alternative, ~5.5GB weights)
VITE_QWEN_MODEL=qwen3-vl:8b

# -----------------------------------------------------------------------------
# ComfyUI Integration (for Image Generation/Draft Previews)
# -----------------------------------------------------------------------------
# ComfyUI server URL (your existing ComfyUI instance)
# NOTE: Image generation features are currently disabled until ComfyUI workflow is implemented
VITE_COMFYUI_URL=http://127.0.0.1:8188

# =============================================================================
# Quick Start Guide
# =============================================================================
#
# 1. Install Ollama from https://ollama.com/download/windows
#    Recommended: Ollama 0.12.7+ (tested on 0.12.11)
#
# 2. Pull the vision model:
#    ollama pull qwen3-vl:8b  (requires ~6.1GB disk, ~8GB VRAM during inference)
#    OR legacy fallback:
#    ollama pull qwen2.5vl:7b (requires ~6GB disk, ~10GB VRAM)
#
# 3. Verify Ollama is running:
#    curl http://localhost:11434/v1/models
#
# 4. Copy this file to .env.local
#
# 5. Run: npm run dev
#
# =============================================================================
# Troubleshooting
# =============================================================================
#
# Qwen3-VL notes:
#   - Requires Ollama 0.12.7+ (qwen3-vl architecture support)
#   - Uses thinking mode by default; the app handles this automatically
#   - Images must be aligned to 32px patches (handled by aiProviderService)
#
# If inference seems slow or VRAM is tight:
#   1. Check GPU usage: nvidia-smi
#   2. Try quantized KV cache to save VRAM:
#      $env:OLLAMA_KV_CACHE_TYPE="q8_0"; ollama serve
#   3. Enable Ollama debug logging:
#      $env:OLLAMA_DEBUG="1"; ollama serve
#
# =============================================================================
