# =============================================================================
# AI Provider Configuration (Qwen via Ollama)
# =============================================================================

# Ollama Server URL (OpenAI-compatible endpoint)
# Default: http://localhost:11434/v1 for Ollama
VITE_QWEN_BASE_URL=http://localhost:11434/v1

# Model identifier (must match the model pulled in Ollama)
# Recommended options:
#   - qwen2.5vl:7b   (Superior reasoning, but may crash on some image dimensions)
#   - minicpm-v:8b   (More stable alternative)
VITE_QWEN_MODEL=minicpm-v:8b

# -----------------------------------------------------------------------------
# ComfyUI Integration (for Image Generation/Draft Previews)
# -----------------------------------------------------------------------------
# ComfyUI server URL (your existing ComfyUI instance)
# NOTE: Image generation features are currently disabled until ComfyUI workflow is implemented
VITE_COMFYUI_URL=http://127.0.0.1:8188

# =============================================================================
# Quick Start Guide
# =============================================================================
#
# 1. Install Ollama from https://ollama.com/download/windows
#
# 2. Pull a vision model:
#    ollama pull minicpm-v:8b
#    OR
#    ollama pull qwen2.5vl:7b  (requires ~17GB VRAM)
#
# 3. Verify Ollama is running:
#    curl http://localhost:11434/v1/models
#
# 4. Copy this file to .env.local
#
# 5. Run: npm run dev
#
# =============================================================================
# Troubleshooting
# =============================================================================
#
# If qwen2.5vl:7b crashes with GGML errors:
#   - Switch to minicpm-v:8b instead (more stable)
#   - Or enable Ollama debug logging:
#     $env:OLLAMA_DEBUG="1"; ollama serve
#
# =============================================================================
