# =============================================================================
# AI Provider Configuration
# =============================================================================

# Choose your AI provider: 'gemini' (cloud) or 'qwen-local' (local vLLM server)
VITE_AI_PROVIDER=qwen-local

# -----------------------------------------------------------------------------
# Option 1: Google Gemini (Cloud)
# -----------------------------------------------------------------------------
# VITE_AI_PROVIDER=gemini
# VITE_API_KEY=your-gemini-api-key-here

# -----------------------------------------------------------------------------
# Option 2: Qwen2.5-VL via vLLM (Local)
# -----------------------------------------------------------------------------
# VITE_AI_PROVIDER=qwen-local

# vLLM Server URL (OpenAI-compatible endpoint)
# Default: http://localhost:8000/v1 (WSL2 accessible from Windows)
VITE_QWEN_BASE_URL=http://localhost:8000/v1

# Model identifier (must match the model loaded in vLLM)
# Options:
#   - Qwen/Qwen2.5-VL-7B-Instruct  (16GB VRAM, best quality)
#   - Qwen/Qwen2.5-VL-3B-Instruct  (8GB VRAM, faster)
#   - Qwen/Qwen2.5-VL-72B-Instruct (48GB+ VRAM, production)
VITE_QWEN_MODEL=Qwen/Qwen2.5-VL-7B-Instruct

# -----------------------------------------------------------------------------
# ComfyUI Integration (for Image Generation/Draft Previews)
# -----------------------------------------------------------------------------
# ComfyUI server URL (your existing ComfyUI instance)
VITE_COMFYUI_URL=http://127.0.0.1:8188

# =============================================================================
# Quick Start Guide
# =============================================================================
#
# FOR QWEN LOCAL SETUP:
#
# 1. Start WSL2 Ubuntu terminal
#
# 2. Launch vLLM server:
#    source ~/vllm-env/bin/activate
#    vllm serve Qwen/Qwen2.5-VL-7B-Instruct \
#      --host 0.0.0.0 \
#      --port 8000 \
#      --gpu-memory-utilization 0.90 \
#      --trust-remote-code
#
# 3. Verify server is running:
#    curl http://localhost:8000/v1/models
#
# 4. Start ComfyUI (if using for image generation):
#    python main.py --listen
#
# 5. Copy this file to .env.local and set VITE_AI_PROVIDER=qwen-local
#
# 6. Run: npm run dev
#
# =============================================================================
